{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function # Python2 compatibility\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from The_Payne import utils\n",
    "from The_Payne import training\n",
    "\n",
    "import torch\n",
    "from The_Payne import spectral_model\n",
    "from The_Payne import fitting\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhangxu/anaconda3/lib/python3.7/site-packages/The_Payne/utils.py\n"
     ]
    }
   ],
   "source": [
    "print(utils.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = utils.load_wavelength_array(survey='galah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14304,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavelength.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model_grid has the following keywords:\n",
      "dict_keys(['wavelength', 'teff', 'logg', 'feh', 'alpha_fe', 'c_fe', 'smod'])\n",
      "Whole training set: 1000\n",
      "Redefined training set (800) and validation set (200)\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "training_labels, training_spectra, validation_labels, validation_spectra = \\\n",
    "utils.load_training_data(survey='galah',size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model_grid has the following keywords:\n",
      "dict_keys(['wavelength', 'teff', 'logg', 'feh', 'alpha_fe', 'c_fe', 'smod'])\n"
     ]
    }
   ],
   "source": [
    "galah_model_grid = utils.get_model_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "interpolated_training_spectra = np.array([scipy.interp(wavelength, galah_model_grid['wavelength'], training_spectra[x]) for x in range(800)])\n",
    "interpolated_validation_spectra = np.array([scipy.interp(wavelength, galah_model_grid['wavelength'], validation_spectra[x]) for x in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = training_labels.astype(np.float64)\n",
    "training_spectra = (interpolated_training_spectra).astype(np.float64)\n",
    "validation_labels = validation_labels.astype(np.float64)\n",
    "validation_spectra = (interpolated_validation_spectra).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 14304)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_spectra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for vision 1.0 \n",
    "#training_loss, validation_loss = training.neural_net(training_labels, training_spectra,\\\n",
    "#                                                     validation_labels, validation_spectra,\\\n",
    "#                                                     num_neurons = 300, num_steps=1e5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: training loss = 9518.314 validation loss = 9463.604\n",
      "iter 100: training loss = 703.312 validation loss = 716.120\n",
      "iter 200: training loss = 459.102 validation loss = 508.559\n",
      "iter 300: training loss = 374.871 validation loss = 389.295\n",
      "iter 400: training loss = 298.588 validation loss = 351.495\n",
      "iter 500: training loss = 229.832 validation loss = 283.768\n",
      "iter 600: training loss = 177.689 validation loss = 219.601\n",
      "iter 700: training loss = 166.938 validation loss = 189.061\n",
      "iter 800: training loss = 153.733 validation loss = 168.728\n",
      "iter 900: training loss = 152.443 validation loss = 156.026\n",
      "iter 1000: training loss = 137.963 validation loss = 146.899\n",
      "iter 1100: training loss = 123.589 validation loss = 138.171\n",
      "iter 1200: training loss = 117.212 validation loss = 130.132\n",
      "iter 1300: training loss = 107.694 validation loss = 122.861\n",
      "iter 1400: training loss = 94.144 validation loss = 117.275\n",
      "iter 1500: training loss = 104.384 validation loss = 112.880\n",
      "iter 1600: training loss = 85.602 validation loss = 109.343\n",
      "iter 1700: training loss = 85.328 validation loss = 106.420\n",
      "iter 1800: training loss = 80.449 validation loss = 104.100\n",
      "iter 1900: training loss = 82.984 validation loss = 101.381\n",
      "iter 2000: training loss = 82.756 validation loss = 99.561\n",
      "iter 2100: training loss = 73.415 validation loss = 99.927\n",
      "iter 2200: training loss = 74.677 validation loss = 96.398\n",
      "iter 2300: training loss = 85.602 validation loss = 95.242\n",
      "iter 2400: training loss = 79.103 validation loss = 95.443\n",
      "iter 2500: training loss = 78.840 validation loss = 93.217\n",
      "iter 2600: training loss = 67.701 validation loss = 92.232\n",
      "iter 2700: training loss = 72.121 validation loss = 91.187\n",
      "iter 2800: training loss = 70.804 validation loss = 90.515\n",
      "iter 2900: training loss = 71.020 validation loss = 89.828\n",
      "iter 3000: training loss = 78.698 validation loss = 89.264\n",
      "iter 3100: training loss = 73.015 validation loss = 88.578\n",
      "iter 3200: training loss = 61.629 validation loss = 88.124\n",
      "iter 3300: training loss = 64.724 validation loss = 88.448\n",
      "iter 3400: training loss = 62.706 validation loss = 87.091\n",
      "iter 3500: training loss = 65.991 validation loss = 86.478\n",
      "iter 3600: training loss = 75.488 validation loss = 86.421\n",
      "iter 3700: training loss = 67.594 validation loss = 85.886\n",
      "iter 3800: training loss = 56.812 validation loss = 85.596\n",
      "iter 3900: training loss = 64.668 validation loss = 84.716\n",
      "iter 4000: training loss = 63.101 validation loss = 84.078\n",
      "iter 4100: training loss = 62.535 validation loss = 83.865\n",
      "iter 4200: training loss = 60.724 validation loss = 83.453\n",
      "iter 4300: training loss = 61.222 validation loss = 83.014\n",
      "iter 4400: training loss = 54.193 validation loss = 82.352\n",
      "iter 4500: training loss = 63.729 validation loss = 82.263\n",
      "iter 4600: training loss = 59.214 validation loss = 82.183\n",
      "iter 4700: training loss = 65.265 validation loss = 81.774\n",
      "iter 4800: training loss = 58.940 validation loss = 81.929\n",
      "iter 4900: training loss = 52.681 validation loss = 81.162\n",
      "iter 5000: training loss = 58.514 validation loss = 80.705\n",
      "iter 5100: training loss = 58.792 validation loss = 80.433\n",
      "iter 5200: training loss = 50.193 validation loss = 80.174\n",
      "iter 5300: training loss = 55.264 validation loss = 79.911\n",
      "iter 5400: training loss = 51.792 validation loss = 79.586\n",
      "iter 5500: training loss = 53.889 validation loss = 78.980\n",
      "iter 5600: training loss = 57.124 validation loss = 78.659\n",
      "iter 5700: training loss = 50.804 validation loss = 79.000\n",
      "iter 5800: training loss = 50.529 validation loss = 78.328\n",
      "iter 5900: training loss = 56.008 validation loss = 78.265\n",
      "iter 6000: training loss = 57.601 validation loss = 78.156\n",
      "iter 6100: training loss = 46.545 validation loss = 78.460\n",
      "iter 6200: training loss = 54.979 validation loss = 77.753\n",
      "iter 6300: training loss = 49.426 validation loss = 81.402\n",
      "iter 6400: training loss = 50.204 validation loss = 77.948\n",
      "iter 6500: training loss = 51.885 validation loss = 77.277\n",
      "iter 6600: training loss = 51.206 validation loss = 76.970\n",
      "iter 6700: training loss = 49.594 validation loss = 76.669\n",
      "iter 6800: training loss = 51.372 validation loss = 76.260\n",
      "iter 6900: training loss = 47.597 validation loss = 75.911\n",
      "iter 7000: training loss = 49.944 validation loss = 76.759\n",
      "iter 7100: training loss = 47.510 validation loss = 76.313\n",
      "iter 7200: training loss = 48.444 validation loss = 76.052\n",
      "iter 7300: training loss = 50.170 validation loss = 75.473\n",
      "iter 7400: training loss = 55.020 validation loss = 75.337\n",
      "iter 7500: training loss = 53.733 validation loss = 76.020\n",
      "iter 7600: training loss = 49.664 validation loss = 75.043\n",
      "iter 7700: training loss = 49.728 validation loss = 75.235\n",
      "iter 7800: training loss = 46.763 validation loss = 75.312\n",
      "iter 7900: training loss = 52.948 validation loss = 75.439\n",
      "iter 8000: training loss = 53.589 validation loss = 74.560\n",
      "iter 8100: training loss = 45.486 validation loss = 76.326\n",
      "iter 8200: training loss = 45.475 validation loss = 74.893\n",
      "iter 8300: training loss = 52.475 validation loss = 76.944\n",
      "iter 8400: training loss = 48.955 validation loss = 75.545\n",
      "iter 8500: training loss = 46.339 validation loss = 74.035\n",
      "iter 8600: training loss = 47.542 validation loss = 74.849\n",
      "iter 8700: training loss = 48.104 validation loss = 73.668\n",
      "iter 8800: training loss = 48.336 validation loss = 74.453\n",
      "iter 8900: training loss = 46.152 validation loss = 73.474\n",
      "iter 9000: training loss = 45.549 validation loss = 73.325\n",
      "iter 9100: training loss = 45.973 validation loss = 74.418\n",
      "iter 9200: training loss = 48.614 validation loss = 72.993\n",
      "iter 9300: training loss = 46.772 validation loss = 73.512\n",
      "iter 9400: training loss = 48.515 validation loss = 73.143\n",
      "iter 9500: training loss = 43.936 validation loss = 73.167\n",
      "iter 9600: training loss = 44.455 validation loss = 72.977\n",
      "iter 9700: training loss = 46.579 validation loss = 72.972\n",
      "iter 9800: training loss = 48.045 validation loss = 72.470\n",
      "iter 9900: training loss = 46.546 validation loss = 72.467\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-91eba26b4653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m training_loss, validation_loss = training.neural_net(training_labels, training_spectra, validation_labels, validation_spectra,\\\n\u001b[0;32m----> 2\u001b[0;31m              num_neurons = 300, num_steps=1e4, learning_rate=1e-4, batch_size=200, num_pixel=14304)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "training_loss, validation_loss = training.neural_net(training_labels, training_spectra, validation_labels, validation_spectra,\\\n",
    "             num_neurons = 300, num_steps=1e4, learning_rate=1e-4, batch_size=200, num_pixel=14304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(\"training_loss.npz\") \n",
    "training_loss = tmp[\"training_loss\"]\n",
    "validation_loss = tmp[\"validation_loss\"]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "#plt.ylim(0,9)\n",
    "plt.plot(np.arange(training_loss.size)*1000, training_loss, 'k', lw=0.5, label = 'Training set')\n",
    "plt.plot(np.arange(training_loss.size)*1000, validation_loss, 'r', lw=0.5, label = 'Validation set')\n",
    "plt.legend(loc = 'best', frameon = False, fontsize= 18)\n",
    "plt.xlabel(\"Step\", size=20)\n",
    "plt.ylabel(\"Loss\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################上面是训练模型,训练模型阶段没有使用mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##需要把训练好的模型放到程序里\n",
    "NN_coeffs = utils.read_in_neural_network()\n",
    "w_array_0, w_array_1, w_array_2, b_array_0, b_array_1, b_array_2, x_min, x_max = NN_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_err = 1e-2*np.ones(len(wavelength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = scaled_labels = [5770, 4.44, 0.0,\\\n",
    "                               0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_labels[:] = (real_labels[:]-x_min)/(x_max-x_min) - 0.5\n",
    "print(np.array(scaled_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = torch.from_numpy(np.array(wavelength)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_spec = spectral_model.get_spectrum_from_neural_net(scaled_labels = scaled_labels[:], NN_coeffs = NN_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_spec = utils.doppler_shift(wavelength, real_spec, scaled_labels[-1])   ##这个命令仅仅用来转成tensor格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(wavelength, real_spec, 'k', lw=0.5)\n",
    "#plt.ylim(0.7, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = real_spec + 0.01*np.random.randn(len(real_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01*np.random.randn(len(real_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = utils.load_wavelength_array(survey='galah')\n",
    "\n",
    "#mask = utils.galah_mask(wavelength, sme_like=False, cores_out=True)\n",
    "#mask = utils.galah_mask(wavelength, sme_like=True, cores_out=True)\n",
    "\n",
    "mask =np.ones(len(wavelength),dtype=bool)\n",
    "\n",
    "#mask = utils.load_galah_mask()\n",
    "#mask[np.abs(wavelength-4861.3230)<2.5]=False # do not use core of H_beta\n",
    "#mask[np.abs(wavelength-6562.7970)<2.5]=False # do not use core of H_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov, model_spec = fitting.fit_normalized_spectrum_single_star_model(\\\n",
    "                                norm_spec = data_spec, spec_err = spec_err,\\\n",
    "                                NN_coeffs = NN_coeffs, wavelength = wavelength, mask=mask, p0 = None)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(wavelength, data_spec, 'k', lw=0.5, label = '\"data\" spec')\n",
    "plt.plot(wavelength, model_spec, 'r--', lw=0.5, label = 'best-fit model')\n",
    "plt.xlim(6500, 6700)\n",
    "plt.legend(loc = 'best', frameon = False, fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################上面是用假设参数作试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
